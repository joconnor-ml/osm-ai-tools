{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "colab": {
   "name": "Classification.ipynb",
   "provenance": []
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "8H69OKXZVOYJ"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/joconnor-ml/osm-ai-tools/blob/master/notebooks/Classification.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eaX34ycsVOYR",
    "outputId": "df8e87fe-c4bb-4436-8e66-3437697e8742"
   },
   "source": [
    "#@title Authenticate, Import, Download Data\n",
    "\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "!pip install -q fsspec gcsfs\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import math\n",
    "\n",
    "!mkdir data\n",
    "!gsutil -m rsync -rd gs://osm-object-detector/data ./data\n",
    "!mkdir pretrained_models\n",
    "!gsutil -m rsync -rd gs://osm-object-detector/pretrained_models ./pretrained_models"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UlcuVbsZVOYS"
   },
   "source": [
    "image_df = pd.read_csv(\"data/images.csv\")\n",
    "object_df = pd.read_csv(\"data/object_location_data_clustered.csv\").merge(\n",
    "    image_df, how=\"left\", on=\"cluster_id\", suffixes=(\"\", \"_image\")\n",
    ")\n",
    "image_df\n",
    "\n",
    "w = 1280\n",
    "h = 1280\n",
    "zoom = 17\n",
    "\n",
    "def getPointLatLng(x, y, lat, lng):\n",
    "    parallelMultiplier = math.cos(lat * math.pi / 180)\n",
    "    degreesPerPixelX = 360 / math.pow(2, zoom + 8)\n",
    "    degreesPerPixelY = 360 / math.pow(2, zoom + 8) * parallelMultiplier\n",
    "    pointLat = lat - degreesPerPixelY * ( y - h / 2)\n",
    "    pointLng = lng + degreesPerPixelX * ( x  - w / 2)\n",
    "\n",
    "    return (pointLat, pointLng)\n",
    "\n",
    "image_size = 0.01\n",
    "def get_patch(row):\n",
    "    ne = getPointLatLng(w, 0, row.center_lat_image, row.center_lon_image)\n",
    "    sw = getPointLatLng(0, h, row.center_lat_image, row.center_lon_image)\n",
    "    nw = getPointLatLng(0, 0, row.center_lat_image, row.center_lon_image)\n",
    "    se = getPointLatLng(w, h, row.center_lat_image, row.center_lon_image)\n",
    "    size_lat = ne[0] - se[0]\n",
    "    size_lon = ne[1] - nw[1]\n",
    "    return pd.Series(dict(\n",
    "        y_min = (0.5 - (row.min_lat - row.center_lat_image)/size_lat)+0.06,  # add a small buffer\n",
    "        y_max = (0.5 - (row.max_lat - row.center_lat_image)/size_lat)-0.06,\n",
    "        x_min = (0.5 + (row.min_lon - row.center_lon_image)/size_lon)-0.06,\n",
    "        x_max = (0.5 + (row.max_lon - row.center_lon_image)/size_lon)+0.06,\n",
    "        osm_id = row.osm_id\n",
    "    ))\n",
    "patches = object_df.apply(get_patch, axis=1)\n",
    "patches[\"image_id\"] = object_df[\"image_id\"]\n",
    "patches[\"osm_id\"] = patches[\"osm_id\"].astype(int)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "y_MfTAangUak",
    "outputId": "c9112976-d846-40d1-ee04-bdc576d799c2"
   },
   "source": [
    "patches.head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "c49SZKcjVOYT"
   },
   "source": [
    "image_patches = []\n",
    "patch_ids = []\n",
    "for image_id in image_df.image_id:\n",
    "    image_patches.append(patches.loc[patches[\"image_id\"]==image_id, [\"y_min\", \"x_min\", \"y_max\", \"x_max\"]].values)\n",
    "    patch_ids.append(patches.loc[patches[\"image_id\"]==image_id, \"osm_id\"].values)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WxLN5U4iVOYU"
   },
   "source": [
    "def patch_gen():\n",
    "    for coords in image_patches:\n",
    "        yield coords\n",
    "def patch_id_gen():\n",
    "    for i in patch_ids:\n",
    "        yield i\n",
    "def label_gen():\n",
    "    for p in image_patches:\n",
    "        yield np.ones_like(p[:, 0], dtype=np.uint8) - 1"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 405
    },
    "id": "Rk4c2vcHfDd3",
    "outputId": "d089b070-8715-4e75-d14c-cbeb5e8f3aa7"
   },
   "source": [
    "# Load images and visualize\n",
    "train_image_dir = 'data/images'\n",
    "# Make a Dataset of file names including all the PNG images files in\n",
    "# the relative image directory.\n",
    "filename_dataset = tf.data.Dataset.from_tensor_slices(train_image_dir + \"/\" + image_df.image_id + \".png\")\n",
    "images = filename_dataset.map(lambda x: tf.io.decode_png(tf.io.read_file(x)))\n",
    "bboxes = tf.data.Dataset.from_generator(patch_gen, output_types=tf.float32)\n",
    "bbox_ids = tf.data.Dataset.from_generator(patch_id_gen, output_types=tf.int32)\n",
    "images_and_bboxes = tf.data.Dataset.zip((images, bboxes, bbox_ids))\n",
    "for img, bbox, bbox_id in images_and_bboxes.take(1):\n",
    "    break\n",
    "plt.imshow(img + 127)\n",
    "bbox.numpy(), bbox_id.numpy()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "N1U49ICKo1BM"
   },
   "source": [
    "bboxes_per_image = object_df.shape[0] / image_df.shape[0]\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "d095IRG8VOYW"
   },
   "source": [
    "# generate positives -- grab crops for each bbox\n",
    "IMAGE_SIZE = 224\n",
    "\n",
    "def sample_positives(img, bboxes, bbox_ids):\n",
    "    crops = tf.image.crop_and_resize(\n",
    "        tf.expand_dims(img, axis=0), bboxes, box_indices=tf.zeros_like(bboxes[:, 0], dtype=tf.int32),\n",
    "        crop_size=[IMAGE_SIZE, IMAGE_SIZE], method='bilinear',\n",
    "        extrapolation_value=0, name=None\n",
    "    )\n",
    "    return tf.data.Dataset.zip((\n",
    "        tf.data.Dataset.from_tensor_slices(crops),\n",
    "        tf.data.Dataset.from_tensor_slices(bbox_ids),\n",
    "        tf.data.Dataset.from_tensor_slices([1]).repeat(-1),\n",
    "    ))\n",
    "\n",
    "def sample_negatives(img, boxes, cls):\n",
    "    return {\"image\": tf.cast(tf.image.random_crop(img, size=[IMAGE_SIZE, IMAGE_SIZE, 3]), np.float32), \"bbox_id\": -1, \"label\": 0}\n",
    "\n",
    "positives = images_and_bboxes.flat_map(sample_positives).map(lambda img, box_id, cls: {\"image\": img, \"bbox_id\": box_id, \"label\": cls})\n",
    "# use `repeat` to balance the data\n",
    "negatives = images_and_bboxes.repeat(round(bboxes_per_image)).map(sample_negatives)\n",
    "final_dataset = tf.data.experimental.sample_from_datasets([positives, negatives])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 809
    },
    "id": "KZ75n1vQVOYW",
    "outputId": "b1d8929f-7047-49cc-aecd-1050815cae4d"
   },
   "source": [
    "for row in final_dataset.take(3):\n",
    "    plt.imshow((row[\"image\"].numpy() + 127).astype(np.uint8))\n",
    "    plt.title(f\"{row['bbox_id']}, {row['label']}\")\n",
    "    plt.show()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mNbAWGu2VOYX",
    "outputId": "79e7cd81-b129-48df-a24e-15d054c110ea"
   },
   "source": [
    "# get size of dataset -- since we changed the number of rows dynamically we have to count them in full\n",
    "for i, _ in enumerate(final_dataset.take(-1)):\n",
    "    pass\n",
    "num_samples = i+1\n",
    "num_samples"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xOSjA4rCVOYY"
   },
   "source": [
    "BATCH_SIZE = 128\n",
    "half_the_data = int(num_samples/2)\n",
    "train_ds = final_dataset.take(half_the_data)\n",
    "val_ds = final_dataset.skip(half_the_data)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yMa5cMx3rzw4"
   },
   "source": [
    "def to_tuple(row):\n",
    "    return row[\"image\"], row[\"label\"]\n",
    "\n",
    "def get_model():\n",
    "    module = tf.keras.models.load_model(os.path.join(\"pretrained_models\", \"resisc_224px_rgb_resnet50\"))\n",
    "    module.trainable = True\n",
    "    module.summary()\n",
    "\n",
    "    images = tf.keras.layers.Input((IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "        tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
    "        tf.keras.layers.experimental.preprocessing.RandomContrast(0.1),\n",
    "        tf.keras.layers.experimental.preprocessing.Resizing(256,256),\n",
    "        tf.keras.layers.experimental.preprocessing.RandomTranslation(0.2, 0.2, fill_mode=\"constant\"),\n",
    "        tf.keras.layers.experimental.preprocessing.RandomRotation(2*math.pi, fill_mode=\"constant\"),\n",
    "        tf.keras.layers.experimental.preprocessing.RandomZoom(0.5, fill_mode=\"constant\"),\n",
    "        tf.keras.layers.experimental.preprocessing.CenterCrop(224,224),\n",
    "    ])\n",
    "    features = module(data_augmentation(images))\n",
    "    features = tf.keras.layers.Concatenate(axis=-1)([\n",
    "        tf.keras.layers.GlobalAveragePooling2D()(features),\n",
    "        tf.keras.layers.GlobalMaxPooling2D()(features)\n",
    "    ])\n",
    "    features = tf.keras.layers.Dropout(0.5)(features)\n",
    "    output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(features)\n",
    "    model = tf.keras.Model(inputs=images, outputs=output)\n",
    "\n",
    "    lr = 0.003 * BATCH_SIZE / 512\n",
    "\n",
    "    # Decay learning rate by a factor of 10 at SCHEDULE_BOUNDARIES.\n",
    "    lr_schedule = tf.keras.optimizers.schedules.PiecewiseConstantDecay(boundaries=[200, 300, 400], \n",
    "                                                                      values=[lr, lr*0.1, lr*0.001, lr*0.0001])\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.9)\n",
    "\n",
    "    model.compile(\n",
    "      optimizer=optimizer,\n",
    "      loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.01),\n",
    "      metrics=['acc']\n",
    "    )\n",
    "    return model\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XrfB2lGoVOYY",
    "outputId": "0d9d6074-3d9b-47ea-ba93-6e78085f4e17"
   },
   "source": [
    "model = get_model()\n",
    "model.fit(train_ds.map(to_tuple).shuffle(1000).batch(BATCH_SIZE).prefetch(2), validation_data=val_ds.map(to_tuple).batch(BATCH_SIZE).prefetch(2), epochs=10)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-TDzwc3_uoIk"
   },
   "source": [
    "pred_df = []\n",
    "for row in val_ds.take(-1):\n",
    "  if row[\"label\"].numpy() == 0:\n",
    "    continue\n",
    "  pred = model.predict(tf.expand_dims(row[\"image\"], axis=0)).item()\n",
    "  pred_df.append({\"pred\": pred, \"label\": row[\"label\"].numpy(), \"osm_id\": row[\"bbox_id\"].numpy()})\n",
    "pred_df = pd.DataFrame(pred_df)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jWMd4LOn5-8C"
   },
   "source": [
    "def plot_one_object(object_id):\n",
    "    filename = patches.loc[patches.osm_id==object_id, \"image_id\"].iloc[0]\n",
    "    img = tf.io.decode_png(tf.io.read_file(f\"data/images/{filename}.png\"))\n",
    "    bboxes = patches.loc[patches.osm_id==object_id, [\"y_min\", \"x_min\", \"y_max\", \"x_max\"]].values\n",
    "    crops = tf.image.crop_and_resize(\n",
    "        tf.expand_dims(img, axis=0), bboxes, box_indices=tf.zeros_like(bboxes[:, 0], dtype=tf.int32),\n",
    "        crop_size=[IMAGE_SIZE, IMAGE_SIZE], method='bilinear',\n",
    "        extrapolation_value=0, name=None\n",
    "    )\n",
    "    plt.imshow((crops[0].numpy() + 127).astype(np.uint8))\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IIWQGqgP8hzm"
   },
   "source": [
    "#@title Plot the 100 most surprising tagged cooling towers\n",
    "for i, row in pred_df.nsmallest(100, \"pred\").iterrows():\n",
    "  plot_one_object(row.osm_id)\n",
    "  plt.title(f\"{row.pred:.3f}, {row.label}, {row.osm_id}\")\n",
    "  plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4Y4LccGE2Xy"
   },
   "source": [
    "I count a few strange square-shaped \"cooling towers\" -- possible these are correct labels, one clear correctly labelled tower and everything else is junk, which is great!\n",
    "\n",
    "Note that some of the images of empty fields etc. may not be real mislabels, but newly constructed cooling towers for which our imagery is out of date. This should be confirmed by a human.\n",
    "\n",
    "Either way, it looks like we have a fairly robust mislabel proposer. Let's finish the job."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KYYidrEwyosd"
   },
   "source": [
    "#@title Train on the second half of objects, predict on the first\n",
    "model = get_model()\n",
    "model.fit(val_ds.map(to_tuple).shuffle(1000).batch(BATCH_SIZE).prefetch(2), validation_data=train_ds.map(to_tuple).batch(BATCH_SIZE).prefetch(2), epochs=10)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wXW9uOW0MmHo"
   },
   "source": [
    "pred_df2 = []\n",
    "for row in train_ds.take(-1):\n",
    "  if row[\"label\"].numpy() == 0:\n",
    "    continue\n",
    "  pred = model.predict(tf.expand_dims(row[\"image\"], axis=0)).item()\n",
    "  pred_df2.append({\"pred\": pred, \"label\": row[\"label\"].numpy(), \"osm_id\": row[\"bbox_id\"].numpy()})\n",
    "pred_df2 = pd.DataFrame(pred_df2)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "t8Jp8sbvVO6n"
   },
   "source": [
    "pred_df2.nsmallest(50, \"pred\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wItDSDgLPbsn"
   },
   "source": [
    "#@title Plot the 100 most surprising tagged cooling towers\n",
    "for i, row in pred_df2.nsmallest(100, \"pred\").iterrows():\n",
    "  plot_one_object(row.osm_id)\n",
    "  plt.title(f\"{row.pred:.3f}, {row.label}, {row.osm_id}\")\n",
    "  plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vLq9rqM-K9cC"
   },
   "source": [
    "df = pd.concat([\n",
    "  pred_df,\n",
    "  pred_df2\n",
    "])\n",
    "df[\"mislabel_score\"] = 1 - df[\"pred\"]\n",
    "df.to_csv(\"gs://osm-object-detector/mislabel_scores.csv\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "souOIRBRVOYa"
   },
   "source": [
    "#@title Bonus: Rudimentary object detection\n",
    "for img in images.take(10):\n",
    "    images = []\n",
    "    fig, ax = plt.subplots(1+(1280//IMAGE_SIZE),1+(1280//IMAGE_SIZE),figsize=(20,20), sharex=True, sharey=True)\n",
    "    fig.tight_layout() # Or equivalently,  \"plt.tight_layout()\"\n",
    "\n",
    "    for i, x in enumerate(range(0, 1260, IMAGE_SIZE)):\n",
    "        for j, y in enumerate(range(0, 1260, IMAGE_SIZE)):\n",
    "            image = np.zeros((IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "            new_image = img.numpy()[y:y+IMAGE_SIZE, x:x+IMAGE_SIZE, :]\n",
    "            image[:new_image.shape[0], :new_image.shape[1], :] = new_image\n",
    "            pred = model.predict(tf.expand_dims(image, axis=0))[0][0]\n",
    "            if pred < 0.25:\n",
    "              image *= 0.25  # darken panels with no detections for emphasis\n",
    "            else:\n",
    "              image *= pred\n",
    "            ax[j][i].imshow(image.astype(np.uint8))\n",
    "            ax[j][i].set_title(pred)\n",
    "    plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OaefYwjbdAnT"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}